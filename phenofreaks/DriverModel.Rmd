---
title: "RandomWalk_Driver_SiteRandomEffect_Model"
author: "Phenofreaks"
date: "11/9/2022"
output: html_document
---

##    Step 0: 
##    Define team name and team members
##    Create functions used in the script
##    Load necessary packages

```{r}
 
team_info <- list(team_name = "Phenofreaks",
                  team_list = list(list(individualName = list(givenName = c("Logan",
                                                                            "Cazimir",
                                                                            "Keo",
                                                                            "Miriam"),
                                                              surName = c("Monks",
                                                                          "Kowalski",
                                                                          "Pangan",
                                                                          "Grady")),
                                        organizationName = "University of Notre Dame",
                                        electronicMailAddress = c("lmonks@nd.edu",
                                                                  "ckowals4@nd.edu",
                                                                  "mgrady7@nd.edu"))))
```

## Download phenocam data function: Originally from the Ecological Forecasting Initiative

```{r}
##' Download Phenocam data
##'
##' @param URL  web address where data is archived
##' @param skipNum The number of lines to skip (22 for 1day file and 17 for roistats file)
##' @export
download.phenocam <- function(URL,skipNum=22) {
  ## check that we've been passed a URL
  if (length(URL) == 1 & is.character(URL) & substr(URL,1,4)=="http") {
    
    ## read data
    dat <- read.csv(URL,skip = skipNum)
    
    ## convert date
    dat$date <- as.Date(as.character(dat$date))
    
    return(dat)
  } else {
    print(paste("download.phenocam: Input URL not provided correctly",URL))
  }
}
```


## Function to calculate phenocam uncertainty: originally from the Ecological Forecasting Initiative
```{r}
calculate.phenocam.uncertainty <- function(dat,dates,target="gcc") {
  sds <- rep(NA,length(dates))
  nboot <- 50
  dat$date = dat$datetime

  for(d in 1:length(dates)){
    dailyDat <- dat[dat$date==dates[d],]
    if(nrow(dailyDat)>0){
      dailyDat <- dailyDat[!is.na(dailyDat[,target]),]
      nrows <- nrow(dailyDat)
      gcc_90s <- rep(NA,nboot)
      for(j in 1:nboot){
        gcc_vec  = pull(dailyDat[,target])
        gcc_90s[j] <- quantile(gcc_vec[sample(x = 1:nrows,size = nrows,replace = T)],0.90)
      }  
      sds[d] <- sd(gcc_90s)
    }else{
      sds[d] <- NA
    }
  }
  return(sds)
}

```

## Load packages
```{r}
library(rjags)
library(dplyr)
library(tidyr)
library(tidybayes)
library(ggplot2)
```

##    Step 1: 
##    Download NEON site meta data, and Neon Phenocam data
##    Manipulate data into formatting appropriate for model fitting and forecasting
Read in site data and most up to date phenology data quantified from phenocam data by the ecoforecasting initiative. We are only interested in deciduous broadleaf forests. This can be filtered in the site_data according to the pheno_cam vegetation type. The remaining sites can be used to filter the phenology data. 

```{r}


site_data <- readr::read_csv("https://raw.githubusercontent.com/eco4cast/neon4cast-targets/main/NEON_Field_Site_Metadata_20220412.csv") |> 
  dplyr::filter(phenology == 1, phenocam_vegetation == "deciduous broadleaf")


NeonPheno =readr::read_csv("https://data.ecoforecast.org/neon4cast-targets/phenology/phenology-targets.csv.gz", guess_max = 1e6) |> 
  na.omit()


#df_past <- neon4cast::noaa_stage3()
#df_future <- neon4cast::noaa_stage2(cycle = 0)
```

## Manipulate phenology data.
1) Remove sites that are not deciduous broadleaf (sites not present in the site_data).
2) Add season column to group data based on spring green up or fall color change and leaf drop

```{r}
Pheno_decid = NeonPheno %>% #Filter sites accord to deciduous broadleaf
  filter(site_id %in% unique(site_data$field_site_id))

Pheno_seasons = Pheno_decid %>% #Add season variable
  mutate(season = case_when(lubridate::month(datetime) <= 6 ~ "spring",
                            lubridate::month(datetime) >= 7 ~ "fall")) 


```

```{r}
past_drivers <- df_past |> 
  dplyr::filter(site_id %in% unique(Pheno_decid$site_id),
                variable %in%  c("precipitation_flux", 
                                 "air_temperature",
                                 "surface_downwelling_shortwave_flux_in_air")
  ) |> 
  dplyr::rename(ensemble = parameter) |>
  dplyr::collect()

past_drivers_ensembles <- past_drivers %>% 
  mutate(date = as.Date(datetime)) %>% 
  group_by(date,variable,site_id,ensemble) %>% 
  summarize(driver_value = mean(prediction, na.rm = TRUE), .groups = "drop") %>% 
  rename(datetime = date)

past_drivers_mean <- past_drivers %>% 
  mutate(date = as.Date(datetime)) %>% 
  group_by(date,variable,site_id) %>% 
  summarize(driver_value = mean(prediction, na.rm = TRUE), .groups = "drop") %>% 
  rename(datetime = date)


rm(df_past)

forecast_date = Sys.Date()
noaa_date = Sys.Date() - lubridate::days(1)

future_drivers <- df_future |> 
  dplyr::filter(site_id %in% unique(Pheno_decid$site_id),
                reference_datetime == as.Date(noaa_date),
                datetime >= lubridate::as_datetime(forecast_date),
                variable %in%  c("precipitation_flux", 
                                 "air_temperature",
                                 "surface_downwelling_shortwave_flux_in_air") 
  ) |>
  dplyr::rename(ensemble = parameter) %>%
  dplyr::select(datetime, prediction, ensemble, variable, site_id) |>
  dplyr::collect()

future_drivers_ensembles <- future_drivers %>% 
  mutate(date = as.Date(datetime)) %>% 
  group_by(date,variable,site_id,ensemble) %>% 
  summarize(driver_value = mean(prediction, na.rm = TRUE), .groups = "drop") %>% 
  rename(datetime = date)


save(past_drivers,past_drivers_ensembles, past_drivers_mean,future_drivers,future_drivers_ensembles, file ="all.sites.drivers.unformatted.RData")
```

```{r}
load("all.sites.drivers.unformatted.RData")
rm(past_drivers)
rm(future_drivers)

past_air_temp_ensembles = past_drivers_ensembles %>% 
  filter(variable == "air_temperature")
past_precip_ensembles = past_drivers_ensembles %>% 
  filter(variable == "precipitation_flux")
past_sunlight_ensembles = past_drivers_ensembles %>%
  filter(variable == "surface_downwelling_shortwave_flux_in_air")


```


## Calculate gcc and rcc uncertainty
Use the function defined above to find the standard deviation in gcc and rcc uncertainty. 

```{r}
Pheno_Wide = Pheno_seasons %>% pivot_wider(names_from="variable",values_from="observation")

gcc.uncert =calculate.phenocam.uncertainty(dat=Pheno_Wide,
                               dates=unique(lubridate::as_date(Pheno_Wide$datetime)),
                               target="gcc_90")
rcc.uncert = calculate.phenocam.uncertainty(dat=Pheno_Wide,
                                          dates=unique(lubridate::as_date(Pheno_Wide$datetime)),
                                          target="rcc_90")
uncert.df = data.frame(datetime=unique(lubridate::as_date(Pheno_Wide$datetime)),
                      gcc_sd = gcc.uncert,
                      rcc_sd = rcc.uncert)
Pheno_Wide$datetime = lubridate::as_date(Pheno_Wide$datetime)
Pheno.long = Pheno_Wide %>% pivot_longer(cols=c("gcc_90","rcc_90"),
                            names_to = c("variable"),
                            values_to="observed")
Pheno.uncert.long = uncert.df %>% pivot_longer(cols=c("gcc_sd","rcc_sd"),
                                               names_to="variable",
                                               values_to = "sd")
Pheno.long$variable = gsub("_90","",Pheno.long$variable)
Pheno.uncert.long$variable = gsub("_sd","",Pheno.uncert.long$variable)
Pheno.uncert = merge(x=Pheno.long,y=Pheno.uncert.long,by=c("datetime","variable"))
```

## Visualize data
Use ggplot to visualize rcc and gcc data during spring green up and fall color change and leaf drop. Can plot across years or choose a single year

```{r, fig.width=10,fig.height=11}

study_sites = c("HARV","SERC","UNDE","GRSM")

Pheno_Wide %>% 
  filter(season == "fall") %>% #spring
  ggplot(aes(x=datetime,y=gcc_90)) +
  geom_point(color="#228b22",size=0.75) + 
  theme_classic() + 
  labs(title="Spring phenology", x = "Date", y= "gcc_90") +
  facet_wrap(~site_id,ncol=5) + 
  theme(strip.background=element_blank(),panel.spacing=unit(1,"lines"))

Pheno_seasons %>% 
  filter(lubridate::year(datetime)== 2021) %>% #2021 YEAR
  filter(season == "fall") %>% # Fall
  ggplot(aes(x=datetime,y=observation,shape=variable)) + #Plot greeness and redness
  geom_point(aes(color=variable),size=0.75) + 
  scale_color_manual(values = c("#daa520","#228b22")) + 
  theme_classic() + 
  facet_wrap(~site_id,ncol=5) +
  labs(title="Fall phenology", x = "Date", y= "Reflectance value",color="Reflectance index",shape="Reflectance index") + 
  theme(strip.background=element_blank(),panel.spacing = unit(1,"lines"))



past_air_temp_ensembles %>%
  filter(site_id %in% study_sites,
         ensemble <= 10,
         lubridate::year(datetime)==2022) %>% 
  ggplot(aes(x=datetime,y=driver_value-273.15,group = as.factor(ensemble), colour=as.factor(ensemble)))+
  geom_line()+
  facet_wrap(~site_id)

past_precip_ensembles %>%
  filter(site_id %in% study_sites,
         ensemble <= 10,
         lubridate::year(datetime)==2022) %>% 
  ggplot(aes(x=datetime,y=driver_value,group = as.factor(ensemble), colour=as.factor(ensemble)))+
  geom_line() + 
  facet_wrap(~site_id)

past_sunlight_ensembles %>%
  filter(site_id %in% study_sites,
         ensemble <= 10,
         lubridate::year(datetime)==2022) %>% 
  ggplot(aes(x=datetime,y=driver_value,group = as.factor(ensemble), colour=as.factor(ensemble)))+
  geom_line()+
  facet_wrap(~site_id)

```

##    Step 2:
##    Calibrate forecasts using bayesian models in Rjags
##    5 models will be present: 
##    1) Random walk model on one site
##    2) Driver model on one site
##    3) Random walk model with drivers on one site
##    4) Random walk model on 4 sites with site level random effect
##    5) Random walk model on 4 sites with site level random effect and drivers


## Model 1: Define a random walk model that would work on one Neon site
```{r}
RandomWalk = "
model{
  # Priors
  x[1] ~ dnorm(x_ic,tau_add)
  tau_obs[1] <- 1 / pow(sd_obs[1], 2)
  y[1] ~ dnorm(x[1],tau_obs[1])
  
  sd_add  ~ dunif(0.000001, 100)
  tau_add <- 1/ pow(sd_add, 2)
  
  # Model
  for(t in 2:N){
    # Process model
    x[t] ~ dnorm(x[t-1], tau_add)
    # Observation precision
    tau_obs[t] <- 1 / pow(sd_obs[t], 2)
    # Data model
    y[t] ~ dnorm(x[t], tau_obs[t])
  }
}
"
```

## Model 2: Define a jags model that works on one site and uses drivers
```{r}

```

## Model 3: Define a random walk model that works on one site and uses drivers
```{r}
RandomWalk_Driver = "
model{
  # Priors
  x[1] ~ dnorm(x_ic,tau_add)
  tau_obs[1] <- 1 / pow(sd_obs[1], 2)
  y[1] ~ dnorm(x[1],tau_obs[1])
  
  sd_add  ~ dunif(0.000001, 100)
  tau_add <- 1/ pow(sd_add, 2)
  
  betaX ~ dnorm(0,0.5)
  betaDriver ~ dnorm(0,0.5)
  
  # Model
  for(t in 2:N){
    # Process model
    mu[t] <- x[t-1]  + betaX*x[t-1] + betaDriver*Driver[t]
    x[t]~dnorm(mu[t],tau_add)
   
    # Observation precision
    tau_obs[t] <- 1 / pow(sd_obs[t], 2)
    # Data model
    y[t] ~ dnorm(x[t], tau_obs[t])
  }
}
"
```


## Model 4: Define a random walk model that has random site effects
```{r}
RandomWalk.SiteRandomEffect = "model{
  
  #### Priors
  
  # Observation precision
  tau_obs[1] <- 1 / pow(sd_obs[1], 2)
  
  # Process precision
  sd_add  ~ dunif(0.000001, 100)
  tau_add <- 1/ pow(sd_add, 2)
  
  # Site random effect precision
  sd_site ~ dunif(0.000001, 100)
  tau_site <- 1/ pow(sd_site, 2)
  
  #### Loop over all sites
  #### X, Y, and Observation precision priors
  #### Nested for loop for data and process models
  #### Site random effect
  for (s in 1:ns){
    
    #### X, Y, and observation priors
    x[1,s] ~ dnorm(x_ic,tau_add)
    y[1,s] ~ dnorm(x[1,s],tau_obs[1])
    
  
    #### Loop over all times: 
    #### 1) Data model 
    #### 2) Process model
    #### 3) Observation precision 
    #### s = site, t = time
    for(t in 2:n){
      
      #### 1) Data model 
      y[t,s] ~ dnorm(x[t,s],tau_obs[t])
      
      #### 2) Process model
      P.new[t,s] = x[t-1,s] + site[s]
      x[t,s] ~ dnorm(P.new[t,s],tau_add)
      
      #### 3) Observation precision
      #tau_obs[t] <- 1 / pow(sd_obs[t], 2)

    }
  
  ## Site random effects
  site[s] ~ dnorm(0,tau_site)
  
  }
  
  for (t in 2:n){
    ## Observation precision
    tau_obs[t] <- 1 / pow(sd_obs[t], 2)

  }
}"

```

## Model 5: Define a random walk model with site random effects model and drivers

```{r}
RandomWalk.Driver.SiteRandomEffect = "model{
  
  #### Priors
  
  # Observation precision
  tau_obs[1] <- 1 / pow(sd_obs[1], 2)
  
  # Process precision
  sd_add  ~ dunif(0.000001, 100)
  tau_add <- 1/ pow(sd_add, 2)
  
  # Site random effect precision
  sd_site ~ dunif(0.000001, 100)
  tau_site <- 1/ pow(sd_site, 2)
  
  # Parameters
  betaX ~ dnorm(0,0.5)
  betaDriver ~ dnorm(0,0.5)
  
  #### Loop over all sites
  #### X, Y, and Observation precision priors
  #### Nested for loop for data and process models
  #### Site random effect
  for (s in 1:ns){
    
    #### X, Y, and observation priors
    x[1,s] ~ dnorm(x_ic,tau_add)
    y[1,s] ~ dnorm(x[1,s],tau_obs[1])
    
  
    #### Loop over all times: 
    #### 1) Data model 
    #### 2) Process model
    #### 3) Observation precision 
    #### s = site, t = time
    for(t in 2:n){
      
      #### 1) Data model 
      y[t,s] ~ dnorm(x[t,s],tau_obs[t])
      
      #### 2) Process model
      #P.new[t,s] = x[t-1,s] + site[s]
      #x[t,s] ~ dnorm(P.new[t,s],tau_add)
      
      mu[t,s] <- x[t-1,s]  + betaX*x[t-1,s] + betaDriver*Driver[t,s] + site[s]
      x[t,s] ~ dnorm(mu[t,s],tau_add)
      
      #### 3) Observation precision
      #tau_obs[t] <- 1 / pow(sd_obs[t], 2)

    }
  
  ## Site random effects
  site[s] ~ dnorm(0,tau_site)
  
  }
  
  for (t in 2:n){
    ## Observation precision
    tau_obs[t] <- 1 / pow(sd_obs[t], 2)

  }
}"


```

## Preparing data for use in multi site models with site level random effects
These models need to have site x time matrices for each variable that differs by site and time. This includes observation measurements for gcc and rcc as well as each of the individual driver variables. Observation error data frame needs prepared as well, but will not have a site x time matrix. 

```{r}
## New pheno data set to use multiple models
phenoDat = Pheno.uncert

## Run just gcc for now on SERC, HARV, UNDE, GRSM
phenoDat= phenoDat %>% filter(variable=="gcc")
phenoDat = phenoDat %>% filter(site_id %in% study_sites) #study sites variable from above

## Set time as a date variable
phenoDat$time <- lubridate::as_date(phenoDat$datetime)

## Forecast length
forecast_length = 35

## Get full time record (earliest Phenocam record to last date of forecast)
time_hist <- tibble::tibble(time = seq(min(phenoDat$time), 
                                       Sys.Date() + lubridate::days(forecast_length), 
                                       by = "1 day"))
## Create a tibble with time, observed data, and observation error
pheno.tib <- tibble::tibble(time = phenoDat$time,
                        p=as.numeric(phenoDat$observed),
                        p.sd=as.numeric(phenoDat$sd),
                        site=phenoDat$site_id)

## Join with full time record to run the model across all dates 
pheno.tib <- dplyr::full_join(pheno.tib, time_hist)

## 


## Get a wide format dataframe for sites x time
## Create observation dataframe 
pheno.obs = pheno.tib[,-3]

## Make wide table format
p.obs.wide = pheno.obs %>% pivot_wider(names_from = "site",values_from ="p")

## Time ordered dataframe
p.obs.wide = p.obs.wide[order(p.obs.wide$time),]

## Remove date and NA column
p.obs.wide = p.obs.wide[,colnames(p.obs.wide) %in% c("SERC","HARV","UNDE","GRSM")]


## Fill in missing observation error data
## Create observation error dataframe
pheno.sd = pheno.tib[,-2]

## Fill non finite values as NA, fill NAs as mean, and fill 0s as min
pheno.sd$p.sd[!is.finite(pheno.sd$p.sd)] <- NA
pheno.sd$p.sd[is.na(pheno.sd$p.sd)] <- mean(pheno.sd$p.sd,na.rm=TRUE)
pheno.sd$p.sd[pheno.sd$p.sd == 0.0] <- min(pheno.sd$p.sd[pheno.sd$p.sd != 0.0])

## Time ordered dataframe
pheno.sd = pheno.sd[order(pheno.sd$time),]
pheno.p.sd = pheno.sd$p.sd[!duplicated(pheno.sd$time)]

## Create data list
data = list(y=as.matrix(p.obs.wide),
            sd_obs = pheno.p.sd,
            n = nrow(p.obs.wide),
            ns = ncol(p.obs.wide),
            x_ic = 0.3,
            sd_site = 0.5)

## Set chain number and seeds
nchain = 3
chain_seeds <- c(200,800,1400)

## Set initial conditions
init <- list()
for(j in 1:nchain){
  init[[j]] <- list(sd_add = 0.3, 
                        #.RNG.name = "base::Wichmann-Hill",
                        #.RNG.seed = as.numeric(chain_seeds[j]),
                        x = data$y)
}

## Initialize jags model
j.model <- jags.model(file = textConnection(RandomWalk.SiteRandomEffect),
                          data = data,
                          inits = init,
                          n.chains = 3)

## Burn in jags model
jags.out   <- coda.samples(model = j.model, variable.names = c("sd_add"), n.iter = 1000)
  
## Diagnostic plots
plot(jags.out)

## Run JAGS model again and sample from the posteriors
m   <- coda.samples(model = j.model,
                      variable.names = c("sd_add","sd_site","site","sd_obs","x","y"),
                      n.iter = 2000,
                      thin = 1)

m.matrix = as.matrix(m)

site_param = m.matrix[,grep("site",colnames(m.matrix))[2:5]]
sd.param = m.matrix[,grep("sd",colnames(m.matrix))]
pred.param = m.matrix[,grep("x",colnames(m.matrix))]


model_sd_obs = m %>% 
  spread_draws(sd_obs[day]) %>%
  rename(ensemble = .iteration) %>%
  mutate(time=time_hist$time[day]) %>%
  ungroup() %>%
  select(time,ensemble,sd_obs)

model_site_param = m %>% 
  spread_draws(site[s]) %>%
  rename(ensemble = .iteration) %>%
  #mutate(site_name=time_hist$time[s]) %>%
  ungroup() %>%
  select(s,ensemble,site)

model_prec_params = m %>% 
  spread_draws(sd_add,sd_site) %>%
  rename(ensemble = .iteration) %>%
  ungroup() %>%
  select(sd_add,sd_site,ensemble)

## Use tidybayes to clean up model output
model_pred = m %>%
      spread_draws(x[day,site],y[day,site]) %>%
      filter(.chain == 1) %>% 
      rename(ensemble = .iteration) %>%
      mutate(time = time_hist$time[day])%>%
      ungroup() %>%
      select(time, x, y, ensemble,site)

## Get credible intervals
model_ci = model_out %>% group_by(time,site) %>%
          summarise(mean.x = mean(x),
                  mean.y = mean(y),
                  upper.x = quantile(x, 0.975),
                  upper.y = quantile(y, 0.975),
                  lower.x = quantile(x, 0.025),
                  lower.y = quantile(y, 0.025),.groups = "drop")

## Change site names back to NEON abbreviations
model_ci$site = gsub(1, "SERC", model_ci$site)
model_ci$site = gsub(2, "HARV", model_ci$site)
model_ci$site = gsub(3, "UNDE", model_ci$site)
model_ci$site = gsub(4, "GRSM", model_ci$site)

## Plot the model output
ggplot(data=model_ci,aes(x = time, y = mean.x)) +
        #geom_point(shape=1) + 
        geom_point(aes(x=time,y=mean.y),color="red",shape=3) +
        geom_ribbon(aes(ymin = lower.x, ymax = upper.x), 
                    alpha = 0.2, color = "lightblue", 
                    fill = "lightblue") +
        #geom_point(data = obs, aes(x = time, y = obs), color = "purple",shape=8) +
        labs(x = "Date", y = "gcc_90") +
        facet_wrap(~as.factor(site))+
        theme_bw()

```

```{r}
past_temp = past_drivers_mean %>% 
  filter(site_id %in% study_sites,
         variable == "air_temperature") %>%
  group_by(datetime,site_id) %>%
  summarise(temperature = mean(driver_value)-273.15,.groups="drop") %>%
  select(datetime,temperature,site_id) %>%
  pivot_wider(names_from = "site_id",values_from="temperature") %>%
  collect()

past_precip = past_drivers_mean %>% 
  filter(site_id %in% study_sites,
         variable == "precipitation_flux") %>%
  group_by(datetime,site_id) %>%
  summarise(precipitation = mean(driver_value),.groups="drop") %>%
  select(datetime,precipitation,site_id) %>%
  pivot_wider(names_from = "site_id",values_from="precipitation") %>%
  collect()

past_sunlight = past_drivers_mean %>% 
  filter(site_id %in% study_sites,
         variable == "surface_downwelling_shortwave_flux_in_air") %>%
  group_by(datetime,site_id) %>%
  summarise(sunlight = mean(driver_value),.groups="drop") %>%
  rename(time=datetime) %>%
  select(time,sunlight,site_id) %>%
  pivot_wider(names_from = "site_id",values_from="sunlight") %>%
  collect()

## Make wide table format
p.obs.wide = pheno.obs %>% pivot_wider(names_from = "site",values_from ="p")

## Time ordered dataframe
p.obs.wide = p.obs.wide[order(p.obs.wide$time),]

## Merge pheno and driver data to match up timing (driver data doesnt begin until september 2020 for some reason)
p.driver.wide = merge(p.obs.wide,past_sunlight,by="time")

## Create data list
data = list(y=as.matrix(p.driver.wide[,2:5]),
            sd_obs = pheno.p.sd,
            n = nrow(p.driver.wide),
            ns = 4,
            Driver = p.driver.wide[,7:10],
            x_ic = 0.3,
            sd_site = 0.5)

## Set chain number and seeds
nchain = 3
chain_seeds <- c(200,800,1400)

## Set initial conditions
init <- list()
for(j in 1:nchain){
  init[[j]] <- list(sd_add = 0.3, 
                        #.RNG.name = "base::Wichmann-Hill",
                        #.RNG.seed = as.numeric(chain_seeds[j]),
                        x = data$y)
}

## Initialize jags model
j.driver.model <- jags.model(file = textConnection(RandomWalk.Driver.SiteRandomEffect),
                          data = data,
                          inits = init,
                          n.chains = 3)

## Burn in jags model
jags.out   <- coda.samples(model = j.driver.model, variable.names = c("sd_add"), n.iter = 1000)
  
## Diagnostic plots
plot(jags.out)

## Run JAGS model again and sample from the posteriors
m.driver   <- coda.samples(model = j.driver.model,
                      variable.names = c("sd_add","sd_site","site","sd_obs","betaX","betaDriver","x","y"),
                      n.iter = 2000,
                      thin = 1)

plot(m.driver[,c(1,2)])


## Use tidybayes to clean up model output
model_pred.driver = m.driver %>%
      spread_draws(x[day,site],y[day,site]) %>%
      filter(.chain == 1) %>% 
      rename(ensemble = .iteration) %>%
      mutate(time = p.driver.wide$time[day])%>%
      ungroup() %>%
      select(time, x, y, ensemble,site)

## Get credible intervals
model_ci.driver = model_pred.driver %>% group_by(time,site) %>%
          summarise(mean.x = mean(x),
                  mean.y = mean(y),
                  upper.x = quantile(x, 0.975),
                  upper.y = quantile(y, 0.975),
                  lower.x = quantile(x, 0.025),
                  lower.y = quantile(y, 0.025),.groups = "drop")

## Change site names back to NEON abbreviations
model_ci.driver$site = gsub(1, "SERC", model_ci.driver$site)
model_ci.driver$site = gsub(2, "HARV", model_ci.driver$site)
model_ci.driver$site = gsub(3, "UNDE", model_ci.driver$site)
model_ci.driver$site = gsub(4, "GRSM", model_ci.driver$site)

## Plot the model output
ggplot(data=model_ci.driver,aes(x = time, y = mean.x)) +
        #geom_point(shape=1) + 
        geom_point(aes(x=time,y=mean.y),color="red",shape=3) +
        geom_ribbon(aes(ymin = lower.x, ymax = upper.x), 
                    alpha = 0.2, color = "lightblue", 
                    fill = "lightblue") +
        #geom_point(data = obs, aes(x = time, y = obs), color = "purple",shape=8) +
        labs(x = "Date", y = "gcc_90") +
        facet_wrap(~as.factor(site))+
        theme_bw()


```

```{r}
forecastP <- function(IC,site,Q=0,n=1,NT){
  P <- matrix(NA,n,NT)  ## storage
  P.prev <- IC           ## initialize
  for(t in 1:NT){
    P.new = P.prev + site  
    P[,t] <- rnorm(n,P.new,Q)                         ## predict next step
    P.prev <- P[,t]                                 ## update IC
  }
  return(P)
}
```

## Deterministic prediction
```{r}
pred.param[1:5,10130:10132]
ncol(pred.param)
forecastP(IC=mean(pred.param[2533,4]),
                  site=mean(site_param[,4]),
                  Q=0,
                  n=1,
                  NT=35)
```

## Ensemble model without data assimilation. 
## Model for each site individually rather than together
```{r}
library(ggplot2)

## Set sites that will be modeled and used in for loop
sites <- unique(as.character(phenoDat$site_id))

## Set target variables of interest
target_variables <- c("gcc","rcc")

## Set forecast length to 35 days
forecast_length <- 35

## Create an array filled with NAs for predictions
predictions <- array(NA, dim = c(length(target_variables), forecast_length, length(sites), 2000))

## Create a place to store saved forecast data
forecast_saved_ensemble <- NULL

## Team name
team_name = "Phenofreaks"

## Loop through target variables - i.e. model both rcc and gcc
for(i in 1:length(target_variables)){
  
  ## Loop through sites of interest - i.e. model each NEON site individually
  for(s in 1:length(sites)){
    
    message(paste0("forecasting ",target_variables[i]," at site: ",sites[s]))
    message("site ", s, " of ", length(sites))
    
    ## Get the data for specific site s in the loop 
    sitePhenoDat <- phenoDat[phenoDat$site_id==sites[s],]
    
    ## Set the time as a date
    sitePhenoDat$time <- lubridate::as_date(sitePhenoDat$datetime)

    # Our forecast start date
    start_forecast <- Sys.Date() + lubridate::days(1)
     
    ################################################## 
    ## change 1 to i in order to forecast gcc and rcc.  
    sitePhenoDat_variable <- sitePhenoDat %>% 
      filter(variable == target_variables[1])
    #sitePhenoDat_sd <- sitePhenoDat %>% 
    #  filter(variable == target_variables_sd[1])
    
    full_time <- tibble::tibble(time = seq(min(sitePhenoDat$time), Sys.Date()  + lubridate::days(forecast_length), by = "1 day"))
    
    
    forecast_start_index <- which(full_time$time == max(sitePhenoDat$time) + lubridate::days(1))
    
    
    ## Create a tibble, 
    d <- tibble::tibble(time = sitePhenoDat_variable$time,
                        p=as.numeric(sitePhenoDat_variable$observed),
                        p.sd=as.numeric(sitePhenoDat_variable$sd))
    
    d <- dplyr::full_join(d, full_time)
    ggplot(d, aes(x = time, y = p)) +
      geom_point()
    
    
    #gap fill the missing precisions by assigning them the average sd for the site
    d$p.sd[!is.finite(d$p.sd)] <- NA
    d$p.sd[is.na(d$p.sd)] <- mean(d$p.sd,na.rm=TRUE)
    d$p.sd[d$p.sd == 0.0] <- min(d$p.sd[d$p.sd != 0.0])
    d$N <- length(d$p)
    data <- list(y = d$p,
                 sd_obs = d$p.sd,
                 N = length(d$p),
                 x_ic = 0.3)
    which(is.na(data$y))
    init_x <- approx(x = d$time[!is.na(d$p)], 
                     y = d$p[!is.na(d$p)], 
                     xout = d$time, rule = 2)$y
    init_x
    #Initialize parameters
    nchain = 3
    chain_seeds <- c(200,800,1400)
    init <- list()
    diff(data$y[!is.na(data$y)])
    diff
    for(j in 1:nchain){
      init[[j]] <- list(sd_add = sd(diff(data$y[!is.na(data$y)])),
                        .RNG.name = "base::Wichmann-Hill",
                        .RNG.seed = chain_seeds[j],
                        x = init_x)
    }
    
    
    
    j.model   <- jags.model(file = textConnection(RandomWalk),
                            data = data,
                            inits = init,
                            n.chains = 3)
    
    
    #Run JAGS model as the burn-in
    jags.out   <- coda.samples(model = j.model, variable.names = c("sd_add"), n.iter = 1000)
    
    #Run JAGS model again and sample from the posteriors
    m   <- coda.samples(model = j.model,
                        variable.names = c("x","sd_add", "y"),
                        n.iter = 2000,
                        thin = 1)
    
    #Use TidyBayes package to clean up the JAGS output
    model_output <- m %>%
      spread_draws(x[day],y[day]) %>%
      filter(.chain == 1) %>%
      rename(ensemble = .iteration) %>%
      mutate(time = full_time$time[day]) %>%
      ungroup() %>%
      select(time, x, y, ensemble)

      #if(generate_plots){
      #Pull in the observed data for plotting
      obs <- tibble(time = d$time,
                    obs = d$p) %>% 
        filter(time >= max(sitePhenoDat$time))
      
        
        ## Get credible intervals and predictions for x and y 
        mod.out = model_output %>%
        group_by(time) %>%
        summarise(mean.x = mean(x),
                  mean.y = mean(y),
                  upper.x = quantile(x, 0.975),
                  upper.y = quantile(y, 0.975),
                  lower.x = quantile(x, 0.025),
                  lower.y = quantile(y, 0.025),.groups = "drop") #%>%
        #filter(time >= max(sitePhenoDat$time) )
        ggplot(data=mod.out,aes(x = time, y = mean.x)) +
        #geom_point(shape=1) + 
        geom_point(aes(x=time,y=mean.y),color="red",shape=3) +
        geom_ribbon(aes(ymin = lower.x, ymax = upper.x), 
                    alpha = 0.2, color = "lightblue", 
                    fill = "lightblue") +
        geom_point(data = obs, aes(x = time, y = obs), color = "purple",shape=8) +
        labs(x = "Date", y = "gcc_90") +
        theme_bw()
      
      #Post past and future
      ggsave(paste0("phenology_",sites[s],"_figure.pdf"), device = "pdf")
    }
    
    #Filter only the forecasted dates and add columns for required variable
    forecast_saved_tmp <- model_output %>%
      filter(time >= start_forecast) %>%
      rename(predicted = y) %>%
      mutate(site_id = sites[s]) %>%
      mutate(forecast_iteration_id = start_forecast) %>%
      mutate(forecast_project_id = team_name,
             variable =  target_variables[i])
    
  
    # Combined with the previous sites
    forecast_saved_ensemble <- rbind(forecast_saved_ensemble, forecast_saved_tmp)
    
}
```

## Submit Forecast function
## Not using yet


```{r}
"submit_forecast <- function(forecast,team_info,submit=FALSE){
  
  ## Forecast output file name in standards requires for Challenge.  
  ## csv.gz means that it will be compressed
  forecast_file <- paste0("Phenology","-",min(forecast$time),"-",team_info$team_name,".csv.gz")
  
  ## Write csv to disk
  write_csv(forecast, forecast_file)
  
  ## Confirm that output file meets standard for Challenge
  neon4cast::forecast_output_validator(forecast_file)
  
  ## Generate metadata
  model_metadata = list(
    forecast = list(
      model_description = list(
        forecast_model_id =  system("git rev-parse HEAD", intern=TRUE), ## current git SHA
        name = "Random walk model of phenology", 
        type = "empirical",  
        repository = "https://github.com/ckowals/NDbio4cast/Phenofreaks" ## Git repo
      ),
      initial_conditions = list(
        status = "absent"
      ),
      drivers = list(
        status = "absent",
        complexity = 1, #Just air temperature
        propagation = list( 
          type = "ensemble", 
          size = 31) 
      ),
      parameters = list(
        status = "absent"
      ),
      random_effects = list(
        status = "propagates",
        complexity = 1
      ),
      process_error = list(
        status = "propogates"
      ),
      obs_error = list(
        status = "propogates"
      )
    )
  )
  
  metadata_file <- neon4cast::generate_metadata(forecast_file, team_info$team_list, model_metadata)
  
  if(submit){
    neon4cast::submit(forecast_file = forecast_file, metadata = metadata_file, ask = FALSE)
  }
  
}"
```

